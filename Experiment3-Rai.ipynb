{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: Umad ul hassan Rai\n",
    "\n",
    "# Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import concat, col, lit, split, udf, size, lit\n",
    "from pyspark.sql import functions as sf\n",
    "from operator import add\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import time\n",
    "import csv\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise Spark Session, setting broadcast Timeout to 36000. \n",
    "sparkSession = SparkSession.builder.appName(\"Exercise1\").config(\"spark.sql.broadcastTimeout\", \"36000\").getOrCreate()\n",
    "sc = sparkSession.sparkContext\n",
    "sqlC = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.1 Vectors Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('80546',\n",
       "  \"the arbitrariness of the genetic code the genetic code has been regarded as arbitrary in the sense that the codon-amino acid assignments could be different than they actually are. this general idea has been spelled out differently by previous, often rather implicit accounts of arbitrariness. they have drawn on the frozen accident theory, on evolutionary contingency, on alternative causal pathways, and on the absence of direct stereochemical interactions between codons and amino acids. it has also been suggested that the arbitrariness of the genetic code justifies attributing semantic information to macromolecules, notably to {dna}. i argue that these accounts of arbitrariness are unsatisfactory. i propose that the code is arbitrary in the sense of jacques monod's concept of chemical arbitrariness: the genetic code is arbitrary in that any codon requires certain chemical and structural properties to specify a particular amino acid, but these properties are not required in virtue of a principle of chemistry. this notion of arbitrariness is compatible with several recent hypotheses about code evolution. i maintain that the code's chemical arbitrariness is neither sufficient nor necessary for attributing semantic information to nucleic acids.\")]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using csv to parse the papers.CSV file to map paper id and abstract as Key/Value\n",
    "# Also allows to replace null bytes in csv file.\n",
    "# Creating paperTermsRDD\n",
    "# Concatenating the title and abstract fields[13] + \" \" + fields[14]\n",
    "rdd2 = sparkSession.sparkContext.textFile(\"papers.csv\")\n",
    "paperTermsRDD = rdd2.map(lambda line: next(csv.reader(x.replace(\"\\x00\", \"\") for x in [line]))) \\\n",
    "                    .map(lambda fields: (fields[0], fields[13] + \" \" + fields[14]))\n",
    "paperTermsRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Paper_Id|                Text|\n",
      "+--------+--------------------+\n",
      "|   80546|the arbitrariness...|\n",
      "+--------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting RDD to DF.\n",
    "sentenceDataFrame = paperTermsRDD.toDF()\n",
    "sentenceDataFrame = sentenceDataFrame.withColumnRenamed(\"_1\", \"Paper_Id\").withColumnRenamed(\"_2\", \"Text\")\n",
    "sentenceDataFrame.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Using regex tokenizer to remove all non-word english characters except (-) and (_)\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Text\", outputCol=\"Words\", pattern=\"[^a-z\\_\\-A-Z]\")\n",
    "\n",
    "# Counting tokens just for sake of seeing length of papers.\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized = regexTokenized.select(\"Paper_Id\", \"Words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"Words\")))\n",
    "regexTokenized.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to remove (-) and (_) from words\n",
    "def remove_hyp_uscore(x):\n",
    "    final = []\n",
    "    for word in x:\n",
    "        removed = word.replace(\"-\",\"\").replace(\"_\",\"\")\n",
    "        final.append(removed)\n",
    "    return final\n",
    "\n",
    "# Creating function to remove words with length smaller than 3 to be used with udf\n",
    "def rem_2len(x):\n",
    "    final = []\n",
    "    for word in x:\n",
    "        if len(word)>2:\n",
    "            final.append(word)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating udf for removing (-) and (_), applying it on respective df\n",
    "rem_hyp_udf = udf(remove_hyp_uscore, ArrayType(StringType()))\n",
    "\n",
    "removed_hyp_df = regexTokenized.withColumn(\"Removed_Hyp\", rem_hyp_udf(regexTokenized[\"words\"])).\\\n",
    "                    select(\"Paper_Id\",\"Removed_Hyp\")\n",
    "\n",
    "# Creating udf for removing words less than length 3 and applying it on respective df\n",
    "rem_2len_words = udf(rem_2len, ArrayType(StringType()))\n",
    "\n",
    "removed_2len_df = removed_hyp_df.withColumn(\"Rem_2len\", rem_2len_words(removed_hyp_df[\"Removed_Hyp\"])).\\\n",
    "                    select(\"Paper_Id\", \"Rem_2len\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "removed_hyp_df.show(1,False)\n",
    "removed_2len_df.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords using Spark Stopwords remover.\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"Rem_2len\", outputCol=\"Filtered\")\n",
    "stopwords_removed = remover.transform(removed_2len_df).select(\"Paper_Id\", \"Filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords_removed.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing stemming by using PorterStemmer() from nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#creatig object of PorterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Function for performing stemming of words by iterating over list of each paper\n",
    "def stemming(x):\n",
    "    porter = PorterStemmer()\n",
    "    final = []\n",
    "    for word in x:\n",
    "        stemmed = porter.stem(word)\n",
    "        final.append(stemmed)\n",
    "    return final\n",
    "\n",
    "# Stemming udf\n",
    "stem_udf = udf(stemming, ArrayType(StringType()))\n",
    "\n",
    "# Stemming the DF which i got after removing stopwords.\n",
    "stemmedDF = stopwords_removed.withColumn(\"Stemmed\", stem_udf(stopwords_removed[\"Filtered\"])).select(\"Paper_Id\", \"Stemmed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stemmedDF.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|Paper_Id|Word     |\n",
      "+--------+---------+\n",
      "|80546   |arbitrari|\n",
      "|80546   |genet    |\n",
      "|80546   |code     |\n",
      "|80546   |genet    |\n",
      "|80546   |code     |\n",
      "+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploding stemmed column to get paper_id|word df and can perform groupBy and other agg functions.\n",
    "exploded_stems = stemmedDF.select(stemmedDF.Paper_Id, sf.explode(stemmedDF.Stemmed)).withColumnRenamed(\"col\", \"Word\")\n",
    "exploded_stems.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+\n",
      "|Word   |Papers   |Papers_set|\n",
      "+-------+---------+----------+\n",
      "|aaomega|[1270054]|[1270054] |\n",
      "|abbrev |[758630] |[758630]  |\n",
      "+-------+---------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy on Word, and then collect list and as set so we can perform filtering\n",
    "# of words occuring in more than 10% papers and atleast 20 Papers.\n",
    "grouped = exploded_stems.groupBy(\"Word\").agg(sf.collect_list(\"Paper_Id\"), sf.collect_set(\"Paper_Id\")).\\\n",
    "            withColumnRenamed(\"collect_list(Paper_Id)\", \"Papers\").withColumnRenamed(\"collect_set(Paper_Id)\", \"Papers_set\")\n",
    "grouped.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the size of each paper set so we know in how many papers every word appeared.\n",
    "grouped_withCount = grouped.withColumn(\"Count\", size((\"Papers_set\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+-----+\n",
      "|Word   |Papers   |Papers_set|Count|\n",
      "+-------+---------+----------+-----+\n",
      "|aaomega|[1270054]|[1270054] |1    |\n",
      "+-------+---------+----------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We take size of set collected in previous step as it help us in filtering the words which are in more than \n",
    "# 10% of the paper and also easily filter for words which are atleast in 20 papers later on sorting helps to get top 1000 Words.\n",
    "# Keeping list for now as it can help in preparing the Paper-Term Vector...\n",
    "grouped_withCount.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the words more than 10%, As we know total papers are 172079.\n",
    "# Simply filtering data frame size column which contains the count of \n",
    "# papers each word has appeared in. Instead of sorting i am using filter\n",
    "# Also, Selecting the words which appeared in atleast 20 papers. as we know count\n",
    "# of Papers for each word. We can simply use filters on count.\n",
    "\n",
    "final_words = grouped_withCount.filter(grouped_withCount.Count <= (0.1 * 172079) ).filter(grouped_withCount.Count >= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_words.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_1000 = final_words.orderBy(final_words.Count.desc()).select(\"*\").limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Word: string (nullable = true)\n",
      " |-- Papers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Papers_set: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Count: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_1000.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to Use stemmed DF to create paper Term Vector. Dropping paper_set and Count column \n",
    "# Will use it to create TermVectors by converting it to the df as paper_id| list of words in paper\n",
    "paper_term = final_1000.select(\"Word\", \"Papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Word: string (nullable = true)\n",
      " |-- Papers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paper_term.printSchema()\n",
    "paper_term.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploding the Papers columm so we get 1 on 1 word-paper df\n",
    "exploded_paperTerm = paper_term.select(\"Word\", sf.explode(\"Papers\")).withColumnRenamed(\"col\", \"Papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| Word| Papers|\n",
      "+-----+-------+\n",
      "|activ|8823677|\n",
      "|activ| 138401|\n",
      "+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contains repititive words as we exploded list of papers colleted above because it will\n",
    "# Further help us in counting number of appearance of specific term.\n",
    "exploded_paperTerm.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the exploded_paperTerm by \"Papers\" column and collect all words as list. this will\n",
    "# Give us dataframe as \"Paper\": \"List of words\" appearing in that paper.\n",
    "grouped_paperTerm = exploded_paperTerm.groupBy(\"Papers\").agg(sf.collect_list(\"Word\")).withColumnRenamed(\"collect_list(Word)\", \"Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Papers|Words                                                                                                                                                                                                                                                                                          |\n",
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|100   |[network, simpl, network, build, network, evolv, design, block, complex, motif, scienc]                                                                                                                                                                                                        |\n",
      "|100001|[scheme, site, genomewid, site, easili, significantli, discrimin, better, eukaryot, made, eukaryot, previou, www, identif, though, identif, appli, separ, possibl, neural, signal, train, signal, lower, signal, precis, set, set, network, sequenc, sequenc, predict, predict, predict, avail]|\n",
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_paperTerm.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using count vectorizer to create sparsevector for frequency of each term in a paper.\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"Words\", outputCol=\"features\")\n",
    "model = cv.fit(grouped_paperTerm)\n",
    "\n",
    "result = model.transform(grouped_paperTerm)\n",
    "result.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalModel = result.select(\"Papers\", \"features\").\\\n",
    "            withColumnRenamed(\"features\", \"TermFrequencyVector\").withColumnRenamed(\"Papers\", \"paper_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|paper_id|TermFrequencyVector                                                                                                                                                                                             |\n",
      "+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|100     |(1000,[2,6,17,184,226,406,499,546,790],[3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                   |\n",
      "|100001  |(1000,[2,5,9,14,70,81,88,122,139,262,302,338,342,405,409,414,462,535,550,560,749,805,808,954,961,997],[1.0,2.0,3.0,2.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0])|\n",
      "+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalModel.show(2,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|paper_id|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|100     |(1000,[2,6,17,184,226,406,499,546,790],[6.930449219064565,2.3267173009996065,2.384034690458811,3.222912811986184,3.077116832483401,3.5224532295273403,4.491909164344742,3.8408562005128726,4.375792994177287])                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|100001  |(1000,[2,5,9,14,70,81,88,122,139,262,302,338,342,405,409,414,462,535,550,560,749,805,808,954,961,997],[2.3101497396881885,4.990895626507497,7.505380008506187,4.572901350030087,2.582921972631529,6.205082745193663,9.26746713360406,2.6930372896008796,2.732704594311079,3.1928208653119516,3.7575974355230737,3.305378819670989,3.7294836426984186,7.169305389639924,3.4244701138949973,3.446244492357481,3.642014495043894,3.7931567375595607,3.841694072367504,4.053792393126434,4.26193673459346,4.455637119726534,8.918515002469197,4.461332190775459,4.385371611474357,4.57155045366807])|\n",
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating TF-IDF dataframe.\n",
    "from pyspark.ml.feature import IDF, Tokenizer\n",
    "\n",
    "idf = IDF(inputCol=\"TermFrequencyVector\", outputCol=\"features\")\n",
    "idfModel = idf.fit(finalModel)\n",
    "rescaledData = idfModel.transform(finalModel)\n",
    "\n",
    "rescaledData.select(\"paper_id\", \"features\").show(2,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tf_idf dataframe and renaming features column to ptf_idf\n",
    "tf_idf = rescaledData.select(\"paper_id\", \"features\").withColumnRenamed(\"features\", \"ptf_idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|paper_id|ptf_idf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|100     |(1000,[2,6,17,184,226,406,499,546,790],[6.930449219064565,2.3267173009996065,2.384034690458811,3.222912811986184,3.077116832483401,3.5224532295273403,4.491909164344742,3.8408562005128726,4.375792994177287])                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|100001  |(1000,[2,5,9,14,70,81,88,122,139,262,302,338,342,405,409,414,462,535,550,560,749,805,808,954,961,997],[2.3101497396881885,4.990895626507497,7.505380008506187,4.572901350030087,2.582921972631529,6.205082745193663,9.26746713360406,2.6930372896008796,2.732704594311079,3.1928208653119516,3.7575974355230737,3.305378819670989,3.7294836426984186,7.169305389639924,3.4244701138949973,3.446244492357481,3.642014495043894,3.7931567375595607,3.841694072367504,4.053792393126434,4.26193673459346,4.455637119726534,8.918515002469197,4.461332190775459,4.385371611474357,4.57155045366807])|\n",
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf.show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.3 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=40, maxIter=10, featuresCol=\"TermFrequencyVector\")\n",
    "model = lda.fit(finalModel)\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(5)\n",
    "\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(finalModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Showing topics and transformed finalModel.\n",
    "topics.show(5, truncate=False)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = transformed.select(\"paper_id\", \"topicDistribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- paper_id: string (nullable = false)\n",
      " |-- topicDistribution: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Exercise 3.4 User Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User: string (nullable = true)\n",
      " |-- Papers_ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating Users DataFrame from user_libraries.txt\n",
    "# with two columns user_hash and Papers\n",
    "# Using the same Dataframes as created in previous exercise.\n",
    "# Using two columns as StringType (Later will assign second column (Papers_ID) to array.)\n",
    "schema = StructType([\n",
    "    StructField(\"User\", StringType(), False),\n",
    "    StructField(\"Papers_ID\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Creating DataFrame df for users_libraries\n",
    "userDF = sqlC.read.schema(schema)\\\n",
    "        .option(\"header\", 'False').option(\"delimiter\", \";\").csv(\"users_libraries.txt\")\n",
    "\n",
    "userDF.printSchema()\n",
    "userDF.show(2, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User: string (nullable = true)\n",
      " |-- PapersID: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                User|            PapersID|\n",
      "+--------------------+--------------------+\n",
      "|f05bcffe7951de9e5...|[1158654, 478707,...|\n",
      "|28d3f81251d94b097...|[3929762, 503574,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- User: string (nullable = true)\n",
      " |-- PapersID: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Type casting from str to array so i can apply explode function on papers_id column.\n",
    "def str_to_arr(my_list):\n",
    "    my_list = my_list.split(\",\")\n",
    "    return [x for x in my_list]\n",
    "\n",
    "# String to array udf\n",
    "str_to_arr_udf = udf(str_to_arr,ArrayType(StringType()))\n",
    "\n",
    "# Converting from string to array\n",
    "userDF = userDF.withColumn('PapersID',str_to_arr_udf(userDF[\"Papers_ID\"]))\n",
    "userDF = userDF.drop(\"Papers_ID\")\n",
    "userDF.printSchema()\n",
    "userDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploding PapersID column.\n",
    "exploded_UserDF = userDF.select(\"User\", sf.explode(\"PapersID\")).withColumnRenamed(\"col\", \"paper_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------+\n",
      "|User                            |paper_id|\n",
      "+--------------------------------+--------+\n",
      "|f05bcffe7951de9e5a32fff4a42eb088|1158654 |\n",
      "|f05bcffe7951de9e5a32fff4a42eb088|478707  |\n",
      "+--------------------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_UserDF.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Profiling using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires 2 DFs, 1: paper_id|User_hash, 2: paper_id|ptf_idf\n",
    "# Performs join on paper_id column of both dfs and then sum respective vectors for users.\n",
    "def idf_user_profile(df1, df2, column1, column2):\n",
    "    joined = df1.join(df2, column1)\n",
    "    # Adding the sparseVector for each user's paper \n",
    "    sum_ = udf(lambda v: float(v.values.sum()), DoubleType())\n",
    "    summed = joined.withColumn(\"idf_sum\", sum_(column2))\n",
    "    # Summing all the sparseVectors sum of a user for all papers\n",
    "    all_summed = summed.groupBy(\"User\").sum(\"idf_sum\").withColumnRenamed(\"sum(idf_sum)\", \"total_IDF\")\n",
    "    return all_summed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile_idf = idf_user_profile(exploded_UserDF, tf_idf, \"paper_id\", \"ptf_idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------+\n",
      "|User                            |total_IDF         |\n",
      "+--------------------------------+------------------+\n",
      "|f1e1cd4ff25018273aafc0c68fbb5a2f|23181.273284519546|\n",
      "|188b2723f7349804c6a237f47b089982|724.0752267183952 |\n",
      "|fc0d51c63591e5b0b12289c002b065c7|24986.54222147031 |\n",
      "|236ef17c11f7f43ac467ce810bdfca2f|1831.3183879490953|\n",
      "|9b87c5cc8095466b44b974a676ff39c3|1127.3446720314141|\n",
      "|5e117499b0c8001c963aebbbf11646e2|9157.817445900286 |\n",
      "|5b0c4c6f84f09ea85bdd9528ce8f9e42|9135.996828178533 |\n",
      "|c1f57a368451d12e22abaad7e45c5f1d|10006.843481476979|\n",
      "|4ae30712c901a64fdb7ad84375df3e28|4785.868851030114 |\n",
      "|0d875b8672933b9a5bb121edc560791b|228.85625105158246|\n",
      "+--------------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_profile_idf.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Profiling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires 2 DFs, 1: paper_id|User_hash, 2: paper_id|topicDistribution\n",
    "# Performs join on paper_id column of both dfs and then sum respective vectors for users.\n",
    "def lda_user_profile(df1, df2, column1, column2):\n",
    "    joined = df1.join(df2, column1)\n",
    "    # Adding the sparseVector for each user's paper \n",
    "    sum_ = udf(lambda v: float(v.values.sum()), DoubleType())\n",
    "    summed = joined.withColumn(\"lda_sum\", sum_(column2))\n",
    "    # Summing all the sparseVectors sum of a user for all papers\n",
    "    all_summed = summed.groupBy(\"User\").sum(\"lda_sum\").withColumnRenamed(\"sum(lda_sum)\", \"total_LDA\")\n",
    "    return all_summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile_lda = lda_user_profile(exploded_UserDF, lda, \"paper_id\", \"topicDistribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------------+\n",
      "|User                            |total_LDA        |\n",
      "+--------------------------------+-----------------+\n",
      "|f1e1cd4ff25018273aafc0c68fbb5a2f|168.0            |\n",
      "|188b2723f7349804c6a237f47b089982|3.0              |\n",
      "|fc0d51c63591e5b0b12289c002b065c7|236.0            |\n",
      "|236ef17c11f7f43ac467ce810bdfca2f|9.999999999999998|\n",
      "|9b87c5cc8095466b44b974a676ff39c3|10.0             |\n",
      "+--------------------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_profile_lda.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.5 Sampling and Data-preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Function Sampler\n",
    "# column1 = column to join training and idf df or training and lda df. \"paper_id\"\n",
    "# column2 = column of idf df which will be used for sum. \"ptf-idf\"\n",
    "# column3 = column of lda df which will be used for sum. \"topicDistribution\"\n",
    "def sampler(userDF, tf_idf, lda, column1, column2, column3):\n",
    "    samplee = userDF.sample(False,0.7,0)\n",
    "    exploded_sample = samplee.select(\"User\", sf.explode(\"PapersID\")).withColumnRenamed(\"col\", \"paper_id\")\n",
    "    # Creating a fraction for every user, and set it as 0.8 so i can do stratified Sampling\n",
    "    fraction = exploded_sample.select(\"User\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n",
    "    # Creating a training DF for users. By selecting fraction(0.8) of values for every user from exploded sample.\n",
    "    training = exploded_sample.sampleBy(\"User\", fraction, 0)\n",
    "    # Creating Test DF by subtracting Training DF from Exploded_Sample.\n",
    "    test = exploded_sample.select(\"*\").subtract(training.select(\"*\"))\n",
    "    # Calling idf_user_profiling function\n",
    "    sampled_tr_idf = idf_user_profile(training, tf_idf, column1, column2)\n",
    "    # Calling lda_user_profilig function\n",
    "    sampled_tr_lda = lda_user_profile(training, lda, column1, column3)\n",
    "    \n",
    "    # Writing to file as csv, which can be used in next experiment.\n",
    "    sampled_tr_idf.write.option(\"sep\",\",\").option(\"header\",\"true\").csv(\"TF-IDF\")\n",
    "    sampled_tr_lda.write.option(\"sep\",\",\").option(\"header\",\"true\").csv(\"LDA\")\n",
    "    \n",
    "    return sampled_tr_idf, sampled_tr_lda\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_tr_idf, sampled_tr_lda = sampler(userDF, tf_idf, lda, \"paper_id\", \"ptf_idf\", \"topicDistribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------+\n",
      "|User                            |total_IDF         |\n",
      "+--------------------------------+------------------+\n",
      "|f1e1cd4ff25018273aafc0c68fbb5a2f|18879.434963006985|\n",
      "|fc0d51c63591e5b0b12289c002b065c7|21059.515189072376|\n",
      "|236ef17c11f7f43ac467ce810bdfca2f|1273.2746527179975|\n",
      "|9b87c5cc8095466b44b974a676ff39c3|907.4855924678919 |\n",
      "|5e117499b0c8001c963aebbbf11646e2|7067.655195121627 |\n",
      "|5b0c4c6f84f09ea85bdd9528ce8f9e42|7375.065700218626 |\n",
      "|c1f57a368451d12e22abaad7e45c5f1d|7611.099838311228 |\n",
      "|4ae30712c901a64fdb7ad84375df3e28|3313.55374336235  |\n",
      "|0d875b8672933b9a5bb121edc560791b|228.85625105158246|\n",
      "|f422cf79c5cc56e92da4c5ab618a5e3c|547.6101949031281 |\n",
      "+--------------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_tr_idf.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------+\n",
      "|User                            |total_LDA         |\n",
      "+--------------------------------+------------------+\n",
      "|f1e1cd4ff25018273aafc0c68fbb5a2f|129.0             |\n",
      "|fc0d51c63591e5b0b12289c002b065c7|190.0             |\n",
      "|236ef17c11f7f43ac467ce810bdfca2f|6.999999999999998 |\n",
      "|9b87c5cc8095466b44b974a676ff39c3|8.0               |\n",
      "|5e117499b0c8001c963aebbbf11646e2|44.0              |\n",
      "|5b0c4c6f84f09ea85bdd9528ce8f9e42|41.0              |\n",
      "|c1f57a368451d12e22abaad7e45c5f1d|40.0              |\n",
      "|4ae30712c901a64fdb7ad84375df3e28|15.999999999999998|\n",
      "|0d875b8672933b9a5bb121edc560791b|1.9999999999999998|\n",
      "|f422cf79c5cc56e92da4c5ab618a5e3c|4.0               |\n",
      "+--------------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_tr_lda.show(10,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
